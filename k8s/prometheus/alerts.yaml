apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-alerts
  namespace: monitoring
  labels:
    app: prometheus
data:
  alerts.yml: |
    groups:
    - name: application_alerts
      interval: 30s
      rules:
      # Pod availability alerts
      - alert: PodDown
        expr: up{job="kubernetes-pods"} == 0
        for: 2m
        labels:
          severity: critical
          component: application
        annotations:
          summary: "Pod {{ $labels.pod }} is down"
          description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} has been down for more than 2 minutes."
          
      - alert: PodCrashLooping
        expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
        for: 5m
        labels:
          severity: warning
          component: application
        annotations:
          summary: "Pod {{ $labels.pod }} is crash looping"
          description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is restarting frequently."
          
      # Resource utilization alerts
      - alert: HighCPUUsage
        expr: |
          (sum(rate(container_cpu_usage_seconds_total{pod=~".*node-app.*"}[5m])) by (pod) / 
          sum(container_spec_cpu_quota{pod=~".*node-app.*"} / container_spec_cpu_period{pod=~".*node-app.*"}) by (pod)) * 100 > 80
        for: 5m
        labels:
          severity: warning
          component: application
        annotations:
          summary: "High CPU usage on {{ $labels.pod }}"
          description: "CPU usage is above 80% on pod {{ $labels.pod }} for more than 5 minutes. Current value: {{ $value }}%"
          
      - alert: HighMemoryUsage
        expr: |
          (sum(container_memory_working_set_bytes{pod=~".*node-app.*"}) by (pod) / 
          sum(container_spec_memory_limit_bytes{pod=~".*node-app.*"}) by (pod)) * 100 > 80
        for: 5m
        labels:
          severity: warning
          component: application
        annotations:
          summary: "High memory usage on {{ $labels.pod }}"
          description: "Memory usage is above 80% on pod {{ $labels.pod }} for more than 5 minutes. Current value: {{ $value }}%"
          
      # Application health alerts
      - alert: HighErrorRate
        expr: |
          (sum(rate(http_requests_total{status=~"5.."}[5m])) by (pod) /
          sum(rate(http_requests_total[5m])) by (pod)) * 100 > 5
        for: 5m
        labels:
          severity: critical
          component: application
        annotations:
          summary: "High error rate on {{ $labels.pod }}"
          description: "Error rate is above 5% on pod {{ $labels.pod }}. Current value: {{ $value }}%"
          
      - alert: HighResponseTime
        expr: |
          histogram_quantile(0.95, 
            sum(rate(http_request_duration_seconds_bucket[5m])) by (le, pod)
          ) > 1
        for: 5m
        labels:
          severity: warning
          component: application
        annotations:
          summary: "High response time on {{ $labels.pod }}"
          description: "95th percentile response time is above 1 second on pod {{ $labels.pod }}. Current value: {{ $value }}s"
          
      # Database connectivity alerts
      - alert: DatabaseConnectionFailure
        expr: |
          increase(database_connection_errors_total[5m]) > 10
        for: 2m
        labels:
          severity: critical
          component: database
        annotations:
          summary: "Database connection failures detected"
          description: "More than 10 database connection failures in the last 5 minutes."
          
      # HPA alerts
      - alert: HPAMaxedOut
        expr: |
          kube_horizontalpodautoscaler_status_current_replicas{horizontalpodautoscaler=~".*node-app.*"} >=
          kube_horizontalpodautoscaler_spec_max_replicas{horizontalpodautoscaler=~".*node-app.*"}
        for: 10m
        labels:
          severity: warning
          component: autoscaling
        annotations:
          summary: "HPA has reached maximum replicas"
          description: "HPA {{ $labels.horizontalpodautoscaler }} has been at maximum replicas for 10 minutes. Consider increasing max replicas or investigating high load."
          
    - name: infrastructure_alerts
      interval: 30s
      rules:
      # Node alerts
      - alert: NodeNotReady
        expr: kube_node_status_condition{condition="Ready",status="true"} == 0
        for: 5m
        labels:
          severity: critical
          component: infrastructure
        annotations:
          summary: "Node {{ $labels.node }} is not ready"
          description: "Node {{ $labels.node }} has been in NotReady state for more than 5 minutes."
          
      - alert: NodeHighDiskUsage
        expr: |
          (1 - (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"})) * 100 > 85
        for: 10m
        labels:
          severity: warning
          component: infrastructure
        annotations:
          summary: "High disk usage on node {{ $labels.node }}"
          description: "Disk usage is above 85% on node {{ $labels.node }}. Current value: {{ $value }}%"
          
      # Persistent Volume alerts
      - alert: PersistentVolumeClaimPending
        expr: kube_persistentvolumeclaim_status_phase{phase="Pending"} == 1
        for: 5m
        labels:
          severity: warning
          component: storage
        annotations:
          summary: "PVC {{ $labels.persistentvolumeclaim }} is pending"
          description: "PVC {{ $labels.persistentvolumeclaim }} in namespace {{ $labels.namespace }} has been pending for more than 5 minutes."
          
    - name: prometheus_alerts
      interval: 30s
      rules:
      # Prometheus self-monitoring
      - alert: PrometheusConfigReloadFailed
        expr: prometheus_config_last_reload_successful == 0
        for: 5m
        labels:
          severity: critical
          component: monitoring
        annotations:
          summary: "Prometheus configuration reload failed"
          description: "Prometheus configuration reload has failed. Check Prometheus logs for details."
          
      - alert: PrometheusTSDBWALCorruption
        expr: increase(prometheus_tsdb_wal_corruptions_total[5m]) > 0
        labels:
          severity: critical
          component: monitoring
        annotations:
          summary: "Prometheus TSDB WAL corruption detected"
          description: "Prometheus has detected WAL corruption. Data loss may occur."
